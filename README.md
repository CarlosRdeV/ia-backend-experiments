# ğŸ§  IA Backend Experiments

Repositorio experimental para construir e integrar servicios de IA usando **Spring Boot** y la **API de OpenAI**.  
Ahora con trazabilidad, mÃ©tricas, alertas y una obsesiÃ³n ligeramente tÃ³xica por el monitoreo.

Este proyecto comenzÃ³ como un demo simple... y ahora corre en contenedores, se autoobserva, y si lo dejaras, probablemente te enviarÃ­a un email a las 3â€¯a.m. diciendo â€œalgo estÃ¡ malâ€.

---

## ğŸš€ CaracterÃ­sticas principales

| Funcionalidad         | DescripciÃ³n                                                                 |
|-----------------------|-----------------------------------------------------------------------------|
| ğŸ§  OpenAI + Spring     | Llamadas a OpenAI vÃ­a WebClient, con reintentos y control de errores.       |
| ğŸ§¾ Logs persistentes   | Todos los prompts y respuestas se guardan con `traceId` incluido.           |
| ğŸ” Trazabilidad        | MDC propagado manualmente entre hilos, gracias a una cola asÃ­ncrona.        |
| ğŸ“Š MÃ©tricas            | Procesados, fallidos y latencia de OpenAI, vÃ­a Micrometer + Prometheus.     |
| ğŸ“¡ Observabilidad      | `/actuator/prometheus`, `/actuator/health` y mÃ¡s desde Spring Actuator.     |
| âš ï¸ Alertas integradas  | Configuradas en Grafana Cloud (y lloran con estilo).                        |
| ğŸ§° Docker Ready        | Dockerfile + docker-compose incluido. SÃ­, con Alloy tambiÃ©n.                |
| ğŸ”„ Background Queue    | Una cola con ejecuciÃ³n controlada simula procesamiento asincrÃ³nico real.   |

---

## âš™ï¸ TecnologÃ­as

- Java 21
- Spring Boot 3.2+
- WebFlux (`WebClient`)
- H2 Database
- Micrometer + Prometheus
- Grafana Cloud + Alloy
- Logback (JSON logs)
- Docker

---

## ğŸ› ï¸ ConfiguraciÃ³n del entorno

### 1. Clona el proyecto

```bash
git clone https://github.com/CarlosRdeV/ia-backend-experiments.git
cd ia-backend-experiments

```

### 2. Crea tu archivo `.env`

```bash
cp .env.example .env
```

### 3. Configura tus variables

```env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_URL=https://api.openai.com/v1/chat/completions
```

### 4. Ejecutar con Docker


```bash
./mvnw clean package -DskipTests
docker-compose up --build
```

> AsegÃºrate de tener configurado `JAVA_HOME` y que el archivo `.env` no estÃ© roto emocionalmente.

> AsegÃºrate de tener configurado Alloy para recolecciÃ³n de mÃ©tricas.
Ver config.alloy incluido en el repositorio.
---

## ğŸ“¬ API

### `POST /api/ia/complete`

EnvÃ­a un prompt a OpenAI.  

#### Solicitud:

```json
{
  "prompt": "Dame una idea de startup en 2025"
}
```

#### Respuesta inmediata:

```json
{
  "response": "ğŸ• Prompt recibido. Procesando en background..."
}
```

ğŸ“œ **La respuesta real llega despuÃ©s, y queda registrada en la base de datos (y en los logs estructurados con traceId)**

## âš™ï¸ Â¿Por quÃ© no se retorna la respuesta directamente?

Para evitar sobrecargar el endpoint en caso de mÃºltiples peticiones concurrentes y simular un flujo realista de procesamiento.  
TambiÃ©n porque OpenAI no aprueba mis decisiones de diseÃ±o aparentemente. ğŸ¥²

---

### `POST /api/prompts`

Consulta paginada de prompts procesados.

ParÃ¡metros disponibles:

- page (por defecto 0)
- size (por defecto 10)
- traceId (opcional)

#### Solicitud:

```
  /prompts?page=0&size=5&traceId=abc-123
```
---

## ğŸ“Š Observabilidad
MÃ©tricas disponibles en `/actuator/prometheus:`

- openai_prompts_processed_total
- openai_prompts_failed_total
- openai_latency_millis
- http_server_requests_seconds

Integrado con:

- Grafana Cloud para visualizaciÃ³n
- Alloy para scrape local y forwarding a Prometheus remoto

## ğŸ“š ğŸ“ Archivos clave

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ controller/             # Endpoint REST principal
â”‚   â”œâ”€â”€ service/                # LÃ³gica del negocio (incluye cola asincrÃ³nica)
â”‚   â”œâ”€â”€ model/                  # Entidad PromptLog
â”‚   â”œâ”€â”€ config/                 # Seguridad, mÃ©tricas
â”‚   â””â”€â”€ util/                   # MDC + Runnable Wrapper
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ config.alloy               # Config de Alloy para enviar mÃ©tricas a Grafana Cloud
â”œâ”€â”€ alert-rules.md             # Reglas de alerta
â””â”€â”€ README.md â† este mismo

```

---

## ğŸ§¼ Notas de higiene

- `.env` estÃ¡ en `.gitignore`. Si lo subes por errorâ€¦ ya te advertÃ­.
- Usa `.env.example` para que otros no pierdan la fe ni sus tokens.

---

## ğŸ”ª Â¿Por quÃ©?

Porque no todo proyecto de IA necesita estar montado sobre mil microservicios.  
Y a veces solo quieres saber si **puedes conectarte a OpenAI sin que te respondan con un 429 y una lÃ¡grima virtual**.

---

## â˜ ï¸ Licencia

Este proyecto **no tiene licencia**.  
Ãšsalo bajo tu propio riesgo. Si te explota el CPU, no fue culpa mÃ­a.  
Pero si impresiona a alguien... sÃ­ lo hice yo. 

Solo no me culpes si se convierte en IA autoconsciente y decide juzgar tus prompts.

---

## ğŸ”§ CÃ³mo iniciar localmente

```bash
docker-compose up --build
```

---

## ğŸ¤™ Contacto

Â¿Te gustÃ³? Â¿Te sirviÃ³?
MenciÃ³nalo en tu post de LinkedIn y parecerÃ¡s mÃ¡s pro.
Yo ya lo hice.

---

