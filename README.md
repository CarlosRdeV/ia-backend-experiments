# ğŸ§  IA Backend Experiments

Repositorio experimental para construir e integrar servicios de IA usando **Spring Boot** y la **API de OpenAI**.

Este proyecto se basa en una arquitectura sÃ³lida, moderna y extensible que sirve como punto de partida para ideas futuras, demostracionesâ€¦ o simplemente para impresionar reclutadores con una falsa sensaciÃ³n de control.

---

## ğŸš€ CaracterÃ­sticas

- ğŸ” ConfiguraciÃ³n segura de API Keys mediante `.env`
- ğŸ“¦ Backend modular con estructura limpia (`controller`, `service`, `dto`, `config`)
- ğŸŒ Consumo de la API de OpenAI con WebClient y manejo de errores 429 (aka *â€œel modelo lloraâ€*)
- ğŸ“„ Swagger UI disponible para probar endpoints
- ğŸ³ Dockerfile listo para producciÃ³n (o al menos para presumir en LinkedIn)

---

## âš™ï¸ TecnologÃ­as

- Java 17
- Spring Boot 3.4
- WebFlux (WebClient)
- OpenAI API (`gpt-3.5-turbo`)
- Lombok *(porque escribir getters es de gente sin autoestima)*
- Swagger / OpenAPI
- Docker

---

## ğŸ› ï¸ ConfiguraciÃ³n del entorno

### 1. Clona el proyecto

```bash
git clone https://github.com/CarlosRdeV/ia-backend-experiments.git
cd ia-backend-experiments
```

### 2. Crea tu archivo `.env`

```bash
cp .env.example .env
```

### 3. Configura tus variables

```env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_URL=https://api.openai.com/v1/chat/completions
```

### 4. Ejecuta el proyecto

```bash
sh run-dev.sh
```

> AsegÃºrate de tener configurado `JAVA_HOME` y que el archivo `.env` no estÃ© roto emocionalmente.

---

## ğŸ“¬ Endpoint disponible

### `POST /api/ia/complete`

EnvÃ­a un prompt a OpenAI.  
La respuesta **no es inmediata**, porque implementamos una cola para simular procesamiento asÃ­ncrono (y porque el modelo tiene emociones, aparentemente).

#### Solicitud:

```json
{
  "prompt": "Dame una idea de startup en 2025"
}
```

#### Respuesta:

```json
{
  "response": "ğŸ• Prompt recibido. Procesando en background..."
}
```

ğŸ“œ **La respuesta real se mostrarÃ¡ en los logs**, por ejemplo:

```
âœ… Respuesta OpenAI: Â¡Hola! Â¿En quÃ© puedo ayudarte hoy?
```

---

## âš™ï¸ Â¿Por quÃ© no se retorna la respuesta directamente?

Para evitar sobrecargar el endpoint en caso de mÃºltiples peticiones concurrentes y simular un flujo realista de procesamiento.  
TambiÃ©n porque OpenAI no aprueba mis decisiones de diseÃ±o aparentemente. ğŸ¥²

---

## ğŸ“š Estructura del proyecto

```
â”œâ”€â”€ controller
â”‚   â””â”€â”€ IaController.java
â”œâ”€â”€ service
â”‚   â””â”€â”€ ia
â”‚       â”œâ”€â”€ OpenAiService.java
â”‚       â”œâ”€â”€ OpenAiRequestBuilder.java
â”‚       â””â”€â”€ OpenAiResponseParser.java
â”œâ”€â”€ dto
â”‚   â”œâ”€â”€ PromptRequest.java
â”‚   â””â”€â”€ IaResponse.java
â””â”€â”€ config
    â””â”€â”€ SecurityConfig.java
```

---

## ğŸ§¼ Notas de higiene

- `.env` estÃ¡ en `.gitignore`. Si lo subes por errorâ€¦ ya te advertÃ­.
- Usa `.env.example` para que otros no pierdan la fe ni sus tokens.

---

## ğŸ”ª Â¿Por quÃ©?

Porque no todo proyecto de IA necesita estar montado sobre mil microservicios.  
Y a veces solo quieres saber si **puedes conectarte a OpenAI sin que te respondan con un 429 y una lÃ¡grima virtual**.

---

## â˜ ï¸ Licencia

Este proyecto **no tiene licencia**.  
Ãšsalo bajo tu propio riesgo. Si te explota el CPU, no fue culpa mÃ­a.  
Pero si impresiona a alguien... sÃ­ lo hice yo.

## ğŸ”§ CÃ³mo iniciar localmente

```bash
docker-compose up --build